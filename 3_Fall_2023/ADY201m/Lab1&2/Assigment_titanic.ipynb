{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zYSCSFyBQG"
      },
      "source": [
        "# Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Titanic dataset\n",
        "* https://www.tensorflow.org/datasets/catalog/titanic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzYnybOiyAJC"
      },
      "outputs": [],
      "source": [
        "# Load dataset.\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8zfgCkEyHQv"
      },
      "source": [
        "* Your work start from here to visualise as much information of Titanic dataset as possible. Make relevant comments about this dataset.\n",
        "* Follow chapter 2: End-to-End Machine Learning Project in Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow Concepts, Tools, and Techniques to Build Intelligent... (Aurélien Géron)\n",
        "* You need to make comments about the relationship between label (in this case the label is \"survived\" column) with other features. Whether they have strong relationship or not.\n",
        "* You also can show some graphs to visualise this relationship and their score based on the guideline the chapter 2.\n",
        "* It will provide you some knowledge on how to read a pandas or csv file and how to get initial information about the dataset you will work with.\n",
        "* This Assignment takes up 20% of the this test and I just evaluate how much information you can show and interpret from this data set. So feel free if you can show more than the template.\n",
        "* There should not be identical between groups, otherwise you will not be allowed for this test.\n",
        "* Every student is supposed to understand the code as I will ask randomly a member in your group to explain to me your code, It also affects to your team score. So please join with your group to complete this task successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsBLeAlDyH6o"
      },
      "source": [
        "**Assignmet 2. Build a customised Machine Learning model using Keras to predict dead cases on Titanic.**\n",
        "* https://www.tensorflow.org/tutorials/estimator/linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzKMHTxzyMPs"
      },
      "source": [
        "**Feature Engineering for the Mode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7KrjABuyElb"
      },
      "outputs": [],
      "source": [
        "y_train_df = dftrain[\"survived\"]\n",
        "x_train_df = dftrain.drop(labels = [\"survived\"],axis=1)\n",
        "y_test_df  = dfeval[\"survived\"]\n",
        "x_test_df  = dfeval.drop(labels = [\"survived\"],axis=1)\n",
        "\n",
        "#should be pandas type, right?\n",
        "print(\"Type of x_train_df:\",type(x_train_df))\n",
        "\n",
        "#Show data frame\n",
        "x_train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOFXWK7yRYg"
      },
      "source": [
        "* sex, class, deck, embark_town, and alone are not numberic columns--> convert to numberic columns by some function.\n",
        "* The three most common are as follows:\n",
        "\n",
        "1.   Integer Encoding: Where each unique label is mapped to an integer.\n",
        "2.   One Hot Encoding: Where each label is mapped to a binary vector.\n",
        "1.   Learned Embedding: Where a distributed representation of the categories is learned.\n",
        "\n",
        "*\n",
        "\n",
        "      For instance: \"classes\" columns including [first,second,third]\n",
        "      catergories So if one point in \"class\" column is \"first\"-->\n",
        "      it will convert to a vector of binary looks like this [1,0,0],\n",
        "      we can recognise that only one point in this vector is \"hot\"(1),\n",
        "      it shows the position of label in the label vector. That is why we call One hot Encoding .\n",
        "\n",
        "* Please read Chapter 2 in text book to have more insights into how we deal with complexed features. It might help you obtain better result in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yce6LMCFyE1q"
      },
      "outputs": [],
      "source": [
        "CATERGORY_COLUMNS=[\"sex\",\"class\",\"deck\",\"embark_town\",\"alone\"]\n",
        "NUMBERIC_COLUMNS=[\"age\",\"n_siblings_spouses\",\"parch\",\"fare\"]\n",
        "x_train_catergory_df = x_train_df[CATERGORY_COLUMNS]\n",
        "x_test_catergory_df = x_test_df[CATERGORY_COLUMNS]\n",
        "\n",
        "# Concat by the first axis (row)\n",
        "x_catergory = pd.concat([x_train_catergory_df,x_test_catergory_df],axis=0)\n",
        "\n",
        "# Use SKlearn to utilise OnHotEncoder library\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "\n",
        "# Train One Hot Vectors model, it is necessary to see all data to make sure that\n",
        "# no category is missed\n",
        "cat_encoder.fit(x_catergory)\n",
        "\n",
        "# Convert data to one hot code\n",
        "x_train_catergory_1hot = cat_encoder.transform(x_train_catergory_df)\n",
        "x_test_catergory_1hot = cat_encoder.transform(x_test_catergory_df)\n",
        "\n",
        "# Because one hot code array x_train_catergory_1hot_df is in\n",
        "# sparse format(to reduce redundance data 0)\n",
        "# we need to convert back into dense format\n",
        "x_train_catergory_1hot = x_train_catergory_1hot.toarray()\n",
        "x_test_catergory_1hot = x_test_catergory_1hot.toarray()\n",
        "# Print the result.\n",
        "# Note that the number of columns now is substantially increase\n",
        "# print(x_train_catergory_1hot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCe-lNu8yE5x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cy8JWfyE8v"
      },
      "outputs": [],
      "source": [
        "# Merge One hot data with numberic data.\n",
        "import numpy as np\n",
        "#.Train data\n",
        "x_train_numberic = x_train_df[NUMBERIC_COLUMNS].to_numpy()\n",
        "x_train_transformed = np.concatenate([x_train_numberic,x_train_catergory_1hot],axis = 1)\n",
        "y_train = y_train_df.to_numpy()\n",
        "#. Test data\n",
        "x_test_numberic = x_test_df[NUMBERIC_COLUMNS].to_numpy()\n",
        "x_test_transformed = np.concatenate([x_test_numberic,x_test_catergory_1hot],axis = 1)\n",
        "y_test = y_test_df.to_numpy()\n",
        "x_test_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_pz3mSayE_e"
      },
      "outputs": [],
      "source": [
        "# Include call back class to do early stopping\n",
        "import tensorflow as tf\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callback = myCallback()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because this dataset is too small in term of quantitive so it is not effective if we split \n",
        "train data into train and valid set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IflMISNyFCv"
      },
      "outputs": [],
      "source": [
        "# Base model\n",
        "import tensorflow as tf\n",
        "base_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation ='relu', input_shape = x_train_transformed.shape[1:]),\n",
        "    tf.keras.layers.Dense(10, activation ='relu'),\n",
        "    tf.keras.layers.Dense(1, activation ='sigmoid') #For binary classification, please refer slide to double check\n",
        "])\n",
        "base_model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
        "base_model.fit(x_train_transformed,y_train,callbacks=[callback],epochs = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5hQtQqRyFFf"
      },
      "outputs": [],
      "source": [
        "# Training Accuracy is around 0.8931--> the model is relatively good!\n",
        "# Most importance of the model is to evaluate with test set. Let's do it right now!\n",
        "base_model.evaluate(x_test_transformed,y_test)\n",
        "# After testing, the accuracy reduced to 0.7955, but it is still acceptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAxVjQsvybq1"
      },
      "source": [
        "* Let improve model by feature engineering\n",
        "* We can scale numberic features to a range which will support ML algorithms better.\n",
        "* Two common ways to get all attributes to have the same scale: min-max\n",
        "scaling and standardization.\n",
        "\n",
        "\n",
        "1.   min-max scaling(normalization): Values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min.\n",
        "2.   standardization: First it subtracts the mean value (so standardized\n",
        "values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWkNYzE7yFIm"
      },
      "outputs": [],
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "x_train_numberic_df = x_train_df[NUMBERIC_COLUMNS]\n",
        "x_test_numberic_df = x_test_df[NUMBERIC_COLUMNS]\n",
        "\n",
        "x_numberic_df = pd.concat([x_train_numberic_df,x_test_numberic_df],axis = 0)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(x_numberic_df)\n",
        "\n",
        "x_train_numberic_transformed = scaler.transform(x_train_numberic_df)\n",
        "\n",
        "x_test_numberic_transformed = scaler.transform(x_test_numberic_df)\n",
        "\n",
        "print(x_train_numberic_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv_15IIZyFLh"
      },
      "outputs": [],
      "source": [
        "x_train_catergory_df = x_train_df[CATERGORY_COLUMNS]\n",
        "x_test_catergory_df = x_test_df[CATERGORY_COLUMNS]\n",
        "\n",
        "#Concat by the first axis (row)\n",
        "x_catergory = pd.concat([x_train_catergory_df,x_test_catergory_df],axis=0)\n",
        "\n",
        "#Use SKlearn to utilise OnHotEncoder library\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "\n",
        "#Train One Hot Vectors model, it is necessary to see all data to make sure that\n",
        "# no category is missed\n",
        "cat_encoder.fit(x_catergory)\n",
        "\n",
        "#Convert data to one hot code\n",
        "x_train_catergory_1hot = cat_encoder.transform(x_train_catergory_df)\n",
        "x_test_catergory_1hot = cat_encoder.transform(x_test_catergory_df)\n",
        "\n",
        "# Because one hot code array x_train_catergory_1hot_df is in\n",
        "# sparse format(to reduce redundance data 0)\n",
        "# we need to convert back into dense format\n",
        "x_train_catergory_1hot = x_train_catergory_1hot.toarray()\n",
        "x_test_catergory_1hot = x_test_catergory_1hot.toarray()\n",
        "# Print the result.\n",
        "# Note that the number of columns now is substantially increase\n",
        "#print(x_train_catergory_1hot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH9GFVrwyFPG"
      },
      "outputs": [],
      "source": [
        "# Merge One hot data with numberic data.\n",
        "import numpy as np\n",
        "#.Train data\n",
        "x_train_transformed = np.concatenate([x_train_numberic_transformed,x_train_catergory_1hot],axis = 1)\n",
        "y_train = y_train_df.to_numpy()\n",
        "#. Test data\n",
        "x_test_transformed = np.concatenate([x_test_numberic_transformed,x_test_catergory_1hot],axis = 1)\n",
        "y_test = y_test_df.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usLfb6R_yFSX"
      },
      "outputs": [],
      "source": [
        "base_model_2 = keras.Sequential([\n",
        "    keras.layers.Dense(30, activation ='relu', input_shape = x_train_transformed.shape[1:]),\n",
        "    keras.layers.Dense(10, activation ='relu'),\n",
        "    keras.layers.Dense(1, activation ='sigmoid') # For binary classification, please refer slide to double check\n",
        "])\n",
        "base_model_2.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
        "base_model_2.fit(x_train_transformed,y_train,callbacks=[callback],epochs = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHMRZjtMyFVa"
      },
      "outputs": [],
      "source": [
        "base_model_2.evaluate(x_test_transformed,y_test)\n",
        "# This model is slightly better than the previous one.\n",
        "# Therefore our feature engineering strategy may be work out, but we need to test\n",
        "# several time to make sure it doesn't happen by chance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTLJPFmvyFZI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWSfEdOKyiFV"
      },
      "source": [
        "**Assignment 2.**\n",
        "Your work continue from here.\n",
        "* You need to modify base_model by any techniques that you have learnt from the lectures (even non sequential model is acceptable).\n",
        "* It is not importance that you have to train with  higher accuracy than the base model, but it should show that you understand how to build a DNN model to solve a problem and how to deal with sophisticated dataset.\n",
        "* In addition, please include callback function and tensorboard (learn from session1, session2) to make early stoping and display training history.\n",
        "* I divided the class into 3 groups, so I expect the model of each group is unique. In another word, if you copy the result of other groups, both groups will be considered as failing this assignment.\n",
        "* To test the result, you should run your code successfully in google colab, then you can show me the highest result that you train. The group with the highest test accuracy score will have bonus points for to recognise your work.\n",
        "* Every student is supposed to understand the work of your group because I will ask randomly one student in the group to explain the model.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
